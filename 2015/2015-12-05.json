["1511.08842",{"format":[],"subject":["Computer Science - Learning"],"relation":[],"rights":[],"publisher":[],"source":[],"contributor":[],"identifier":["http://arxiv.org/abs/1511.08842"],"date":["2015-11-27"],"type":["text"],"creator":["Ravishankar, Saiprasad","Nadakuditi, Raj Rao","Fessler, Jeffrey A."],"language":[],"coverage":[],"title":["Efficient Sum of Outer Products Dictionary Learning (SOUP-DIL) - The\n  $\\ell_0$ Method"],"description":["  The sparsity of natural signals and images in a transform domain or\ndictionary has been extensively exploited in several applications such as\ncompression, denoising and inverse problems. More recently, data-driven\nadaptation of synthesis dictionaries has shown promise in many applications\ncompared to fixed or analytical dictionary models. However, dictionary learning\nproblems are typically non-convex and NP-hard, and the usual alternating\nminimization approaches for these problems are often computationally expensive,\nwith the computations dominated by the NP-hard synthesis sparse coding step. In\nthis work, we investigate an efficient method for $\\ell_{0}$ \"norm\"-based\ndictionary learning by first approximating the training data set with a sum of\nsparse rank-one matrices and then using a block coordinate descent approach to\nestimate the unknowns. The proposed block coordinate descent algorithm involves\nefficient closed-form solutions. In particular, the sparse coding step involves\na simple form of thresholding. We provide a convergence analysis for the\nproposed block coordinate descent approach. Our numerical experiments show the\npromising performance and significant speed-ups provided by our method over the\nclassical K-SVD scheme in sparse signal representation and image denoising.\n","Comment: A short version of this work [arXiv:1511.06333] has also been\n  submitted to ICLR 2016"]}]
