["1008.4000",{"format":[],"subject":["Computer Science - Learning","Statistics - Machine Learning"],"relation":[],"rights":[],"publisher":[],"source":[],"contributor":[],"identifier":["http://arxiv.org/abs/1008.4000"],"date":["2010-08-24"],"type":["text"],"creator":["Zhou, Tianyi","Tao, Dacheng","Wu, Xindong"],"language":[],"coverage":[],"title":["NESVM: a Fast Gradient Method for Support Vector Machines"],"description":["  Support vector machines (SVMs) are invaluable tools for many practical\napplications in artificial intelligence, e.g., classification and event\nrecognition. However, popular SVM solvers are not sufficiently efficient for\napplications with a great deal of samples as well as a large number of\nfeatures. In this paper, thus, we present NESVM, a fast gradient SVM solver\nthat can optimize various SVM models, e.g., classical SVM, linear programming\nSVM and least square SVM. Compared against SVM-Perf\n\\cite{SVM_Perf}\\cite{PerfML} (its convergence rate in solving the dual SVM is\nupper bounded by $\\mathcal O(1/\\sqrt{k})$, wherein $k$ is the number of\niterations.) and Pegasos \\cite{Pegasos} (online SVM that converges at rate\n$\\mathcal O(1/k)$ for the primal SVM), NESVM achieves the optimal convergence\nrate at $\\mathcal O(1/k^{2})$ and a linear time complexity. In particular,\nNESVM smoothes the non-differentiable hinge loss and $\\ell_1$-norm in the\nprimal SVM. Then the optimal gradient method without any line search is adopted\nto solve the optimization. In each iteration round, the current gradient and\nhistorical gradients are combined to determine the descent direction, while the\nLipschitz constant determines the step size. Only two matrix-vector\nmultiplications are required in each iteration round. Therefore, NESVM is more\nefficient than existing SVM solvers. In addition, NESVM is available for both\nlinear and nonlinear kernels. We also propose \"homotopy NESVM\" to accelerate\nNESVM by dynamically decreasing the smooth parameter and using the continuation\nmethod. Our experiments on census income categorization, indoor/outdoor scene\nclassification, event recognition and scene recognition suggest the efficiency\nand the effectiveness of NESVM. The MATLAB code of NESVM will be available on\nour website for further assessment.\n","Comment: 10 pages, 11 figures"]}]
["1109.6628",{"format":[],"subject":["Statistics - Applications","Mathematics - Probability"],"relation":[],"rights":[],"publisher":[],"source":[],"contributor":[],"identifier":["http://arxiv.org/abs/1109.6628"],"date":["2011-09-27"],"type":["text"],"creator":["Dominicy, Yves","Ley, Christophe","Swan, Yvik"],"language":[],"coverage":[],"title":["A Stochastic Analysis of Table Tennis"],"description":["  We establish a general formula for the distribution of the score in table\ntennis. We use this formula to derive the probability distribution (and hence\nthe expectation and variance) of the number of rallies necessary to achieve any\ngiven score. We use these findings to investigate the dependence of these\nquantities on the different parameters involved (number of points needed to win\na set, number of consecutive serves, etc.), with particular focus on the rule\nchange imposed in 2001 by the International Table Tennis Federation (ITTF).\nFinally we briefly indicate how our results can lead to more efficient\nestimation techniques of individual players' abilities.\n"]}]
