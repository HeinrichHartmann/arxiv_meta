["1106.1813",{"format":[],"subject":["Computer Science - Artificial Intelligence"],"relation":[],"rights":[],"publisher":[],"source":[],"contributor":[],"identifier":["http://arxiv.org/abs/1106.1813","Journal Of Artificial Intelligence Research, Volume 16, pages\n  321-357, 2002","doi:10.1613/jair.953"],"date":["2011-06-09"],"type":["text"],"creator":["Chawla, N. V.","Bowyer, K. W.","Hall, L. O.","Kegelmeyer, W. P."],"language":[],"coverage":[],"title":["SMOTE: Synthetic Minority Over-sampling Technique"],"description":["  An approach to the construction of classifiers from imbalanced datasets is\ndescribed. A dataset is imbalanced if the classification categories are not\napproximately equally represented. Often real-world data sets are predominately\ncomposed of \"normal\" examples with only a small percentage of \"abnormal\" or\n\"interesting\" examples. It is also the case that the cost of misclassifying an\nabnormal (interesting) example as a normal example is often much higher than\nthe cost of the reverse error. Under-sampling of the majority (normal) class\nhas been proposed as a good means of increasing the sensitivity of a classifier\nto the minority class. This paper shows that a combination of our method of\nover-sampling the minority (abnormal) class and under-sampling the majority\n(normal) class can achieve better classifier performance (in ROC space) than\nonly under-sampling the majority class. This paper also shows that a\ncombination of our method of over-sampling the minority class and\nunder-sampling the majority class can achieve better classifier performance (in\nROC space) than varying the loss ratios in Ripper or class priors in Naive\nBayes. Our method of over-sampling the minority class involves creating\nsynthetic minority class examples. Experiments are performed using C4.5, Ripper\nand a Naive Bayes classifier. The method is evaluated using the area under the\nReceiver Operating Characteristic curve (AUC) and the ROC convex hull strategy.\n"]}]
